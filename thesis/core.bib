
@book{aggarwal_neural_2018,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  shorttitle = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Aggarwal, Charu C.},
  year = {2018},
  month = sep,
  publisher = {{Springer}},
  address = {{Cham, Zug}},
  doi = {10.1007/978-3-319-94463-0},
  file = {/home/rafael/Zotero/storage/WL9KHJK9/Aggarwal - 2018 - Neural Networks and Deep Learning A Textbook.pdf},
  isbn = {978-3-319-94462-3},
  keywords = {used},
  language = {en}
}

@article{aubret_survey_2019,
  title = {A Survey on Intrinsic Motivation in Reinforcement Learning},
  author = {Aubret, Arthur and Matignon, Laetitia and Hassas, Salima},
  year = {2019},
  month = nov,
  abstract = {The reinforcement learning (RL) research area is very active, with an important number of new contributions; especially considering the emergent field of deep RL (DRL). However a number of scientific and technical challenges still need to be addressed, amongst which we can mention the ability to abstract actions or the difficulty to explore the environment which can be addressed by intrinsic motivation (IM). In this article, we provide a survey on the role of intrinsic motivation in DRL. We categorize the different kinds of intrinsic motivations and detail for each category, its advantages and limitations with respect to the mentioned challenges. Additionnally, we conduct an in-depth investigation of substantial current research questions, that are currently under study or not addressed at all in the considered research area of DRL. We choose to survey these research works, from the perspective of learning how to achieve tasks. We suggest then, that solving current challenges could lead to a larger developmental architecture which may tackle most of the tasks. We describe this developmental architecture on the basis of several building blocks composed of a RL algorithm and an IM module compressing information.},
  archivePrefix = {arXiv},
  eprint = {1908.06976},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/RUSYSHXM/Aubret et al. - 2019 - A survey on intrinsic motivation in reinforcement .pdf;/home/rafael/Zotero/storage/23ENZ5UM/1908.html},
  journal = {arXiv:1908.06976},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,used}
}

@book{awad_deep_2015,
  title = {Efficient {{Learning Machines}}},
  author = {Awad, Mariette and Khanna, Rahul},
  year = {2015},
  month = apr,
  publisher = {{Apress}},
  address = {{Berkeley, California}},
  doi = {10.1007/978-1-4302-5990-9},
  abstract = {Deep learning is on the rise in the machine learning community, because the traditional shallow learning architectures have proved unfit for the more challenging tasks of machine learning and strong artificial intelligence (AI). The surge in and wide availability of increased computing power, coupled with the creation of efficient training algorithms and advances in neuroscience, have enabled the implementation, hitherto impossible, of deep learning principles. These developments have led to the formation of deep architecture algorithms that look to cognitive neuroscience to suggest biologically inspired learning solutions. This chapter presents the concepts of spiking neural networks (SNNs) and hierarchical temporal memory (HTM), whose associated techniques are the least mature of the techniques covered in this book.},
  file = {/home/rafael/Zotero/storage/42JJ9TY2/Awad and Khanna - 2015 - Deep learning.pdf},
  isbn = {978-1-4302-5989-3},
  keywords = {used}
}

@article{bellemare_unifying_2016,
  title = {Unifying {{Count}}-{{Based Exploration}} and {{Intrinsic Motivation}}},
  author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  year = {2016},
  month = nov,
  abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.},
  archivePrefix = {arXiv},
  eprint = {1606.01868},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/7KPDT6VM/Bellemare et al. - 2016 - Unifying Count-Based Exploration and Intrinsic Mot.pdf;/home/rafael/Zotero/storage/C4QY6F42/1606.html},
  journal = {arXiv:1606.01868},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,used}
}

@article{bellman_theory_1954,
  title = {The Theory of Dynamic Programming},
  author = {Bellman, Richard},
  year = {1954},
  month = nov,
  volume = {60},
  pages = {503--516},
  issn = {0002-9904},
  doi = {10.1090/S0002-9904-1954-09848-8},
  file = {/home/rafael/Zotero/storage/MYRBSM3N/Bellman - 1954 - The theory of dynamic programming.pdf},
  journal = {Bulletin of the American Mathematical Society},
  keywords = {used},
  language = {en},
  number = {6}
}

@article{burda_exploration_2018,
  title = {Exploration by {{Random Network Distillation}}},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  year = {2018},
  month = oct,
  abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  archivePrefix = {arXiv},
  eprint = {1810.12894},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/3RGH6BAW/Burda et al. - 2018 - Exploration by Random Network Distillation.pdf;/home/rafael/Zotero/storage/JUSA5GYT/1810.html},
  journal = {arXiv:1810.12894},
  keywords = {combining,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Reward Combination,Statistics - Machine Learning}
}

@article{burda_large-scale_2018-1,
  title = {Large-{{Scale Study}} of {{Curiosity}}-{{Driven Learning}}},
  author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  year = {2018},
  month = aug,
  abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
  archivePrefix = {arXiv},
  eprint = {1808.04355},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/SATIVCPM/Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf;/home/rafael/Zotero/storage/RKK3URTF/1808.html},
  journal = {arXiv:1808.04355},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning,used}
}

@article{ecoffet_go-explore_2019,
  title = {Go-{{Explore}}: A {{New Approach}} for {{Hard}}-{{Exploration Problems}}},
  shorttitle = {Go-{{Explore}}},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2019},
  month = may,
  abstract = {A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to encourage exploration and improve performance on hardexploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember states that have previously been visited, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through exploiting any available means (including by introducing determinism), then robustify (create a policy that can reliably perform the solution) via imitation learning. The combined effect of these principles generates dramatic performance improvements on hardexploration problems. On Montezuma's Revenge, without being provided any domain knowledge, Go-Explore scores over 43,000 points, almost 4 times the previous state of the art. Go-Explore can also easily harness human-provided domain knowledge, and when augmented with it Go-Explore scores a mean of over 650,000 points on Montezuma's Revenge. Its max performance of 18 million surpasses the human world record by an order of magnitude, thus meeting even the strictest definition of ``superhuman'' performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean performance of almost 60,000 points also exceeds expert human performance. Because GoExplore can produce many high-performing demonstrations automatically and cheaply, it also outperforms previous imitation learning work in which the solution was provided in the form of a human demonstration. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in a variety of domains, especially the many that often harness a simulator during training (e.g. robotics).},
  archivePrefix = {arXiv},
  eprint = {1901.10995},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/PDHE2RC9/Ecoffet et al. - 2019 - Go-Explore a New Approach for Hard-Exploration Pr.pdf},
  journal = {arXiv:1901.10995},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en}
}

@article{francois-lavet_introduction_2018,
  title = {An {{Introduction}} to {{Deep Reinforcement Learning}}},
  author = {{Francois-Lavet}, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  year = {2018},
  month = mar,
  volume = {11},
  pages = {219--354},
  issn = {1935-8237},
  doi = {10.1561/2200000071},
  abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  file = {/home/rafael/Zotero/storage/UV7ZPW2A/Francois-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf;/home/rafael/Zotero/storage/4DMYB594/1811.html},
  journal = {Foundations and Trends in Machine Learning},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,used},
  number = {3-4}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  publisher = {{The MIT Press}},
  doi = {10.5555/3086952},
  abstract = {"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  isbn = {978-0-262-03561-3},
  keywords = {used}
}

@article{gregor_variational_2016,
  title = {Variational {{Intrinsic Control}}},
  author = {Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
  year = {2016},
  month = nov,
  abstract = {In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.},
  archivePrefix = {arXiv},
  eprint = {1611.07507},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/866J8JW5/Gregor et al. - 2016 - Variational Intrinsic Control.pdf;/home/rafael/Zotero/storage/W4TXTLEZ/1611.html},
  journal = {arXiv:1611.07507},
  keywords = {combining,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Reward Combination}
}

@article{huang_learning_2019,
  title = {Learning {{Gentle Object Manipulation}} with {{Curiosity}}-{{Driven Deep Reinforcement Learning}}},
  author = {Huang, Sandy H. and Zambelli, Martina and Kay, Jackie and Martins, Murilo F. and Tassa, Yuval and Pilarski, Patrick M. and Hadsell, Raia},
  year = {2019},
  month = mar,
  abstract = {Robots must know how to be gentle when they need to interact with fragile objects, or when the robot itself is prone to wear and tear. We propose an approach that enables deep reinforcement learning to train policies that are gentle, both during exploration and task execution. In a reward-based learning environment, a natural approach involves augmenting the (task) reward with a penalty for non-gentleness, which can be defined as excessive impact force. However, augmenting with only this penalty impairs learning: policies get stuck in a local optimum which avoids all contact with the environment. Prior research has shown that combining auxiliary tasks or intrinsic rewards can be beneficial for stabilizing and accelerating learning in sparse-reward domains, and indeed we find that introducing a surprise-based intrinsic reward does avoid the no-contact failure case. However, we show that a simple dynamics-based surprise is not as effective as penalty-based surprise. Penalty-based surprise, based on predicting forceful contacts, has a further benefit: it encourages exploration which is contact-rich yet gentle. We demonstrate the effectiveness of the approach using a complex, tendon-powered robot hand with tactile sensors. Videos are available at http://sites.google.com/view/gentlemanipulation.},
  archivePrefix = {arXiv},
  eprint = {1903.08542},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/JFHQDH6X/Huang et al. - 2019 - Learning Gentle Object Manipulation with Curiosity.pdf;/home/rafael/Zotero/storage/LQMRMPWQ/1903.html},
  journal = {arXiv:1903.08542},
  keywords = {combining,Computer Science - Robotics,Reward Combination}
}

@article{kaelbling_reinforcement_1996,
  title = {Reinforcement {{Learning}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}}},
  author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
  year = {1996},
  month = may,
  volume = {4},
  pages = {237--285},
  issn = {1076-9757},
  doi = {10.1613/jair.301},
  copyright = {Copyright (c)},
  file = {/home/rafael/Zotero/storage/WJ4X4NMT/Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf;/home/rafael/Zotero/storage/BN7TSXB9/10166.html},
  journal = {Journal of Artificial Intelligence Research},
  keywords = {used},
  language = {en}
}

@inproceedings{kim_curiosity-bottleneck_2019-1,
  title = {Curiosity-{{Bottleneck}}: {{Exploration By Distilling Task}}-{{Specific Novelty}}},
  shorttitle = {Curiosity-{{Bottleneck}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Youngjin and Nam, Wontae and Kim, Hyunwoo and Kim, Ji-Hoon and Kim, Gunhee},
  year = {2019},
  month = may,
  volume = {97},
  pages = {3379--3388},
  address = {{Long Beach, Claifornia}},
  issn = {1938-7228},
  doi = {missing},
  abstract = {Exploration based on state novelty has brought great success in challenging reinforcement learning problems with sparse rewards. However, existing novelty-based strategies become inefficient in rea...},
  chapter = {Machine Learning},
  file = {/home/rafael/Zotero/storage/7IKXV4Z8/Kim et al. - 2019 - Curiosity-Bottleneck Exploration By Distilling Ta.pdf;/home/rafael/Zotero/storage/ESUJIDY8/kim19c.html},
  language = {en}
}

@article{kim_emi_2019,
  title = {{{EMI}}: {{Exploration}} with {{Mutual Information}}},
  shorttitle = {{{EMI}}},
  author = {Kim, Hyoungseok and Kim, Jaekyeom and Jeong, Yeonwoo and Levine, Sergey and Song, Hyun Oh},
  year = {2019},
  month = jun,
  abstract = {Reinforcement learning algorithms struggle when the reward signal is very sparse. In these cases, naive random exploration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or discriminative modeling of novelty. We propose EMI, which is an exploration method that constructs embedding representation of states and actions that does not rely on generative decoding of the full observation but extracts predictive signals that can be used to guide exploration based on forward prediction in the representation space. Our experiments show competitive results on challenging locomotion tasks with continuous control and on image-based exploration tasks with discrete actions on Atari. The source code is available at https://github.com/snu-mllab/EMI .},
  archivePrefix = {arXiv},
  eprint = {1810.01176},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/RV6VPRB8/Kim et al. - 2019 - EMI Exploration with Mutual Information.pdf;/home/rafael/Zotero/storage/MR38H97F/1810.html},
  journal = {arxiv:1810.01176},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,used}
}

@article{linke_adapting_2020,
  title = {Adapting {{Behaviour}} via {{Intrinsic Reward}}: {{A Survey}} and {{Empirical Study}}},
  shorttitle = {Adapting {{Behaviour}} via {{Intrinsic Reward}}},
  author = {Linke, Cam and Ady, Nadia M. and White, Martha and Degris, Thomas and White, Adam},
  year = {2020},
  month = may,
  abstract = {Learning about many things can provide numerous benefits to a reinforcement learning system. For example, learning many auxiliary value functions, in addition to optimizing the environmental reward, appears to improve both exploration and representation learning. The question we tackle in this paper is how to sculpt the stream of experience---how to adapt the learning system's behavior---to optimize the learning of a collection of value functions. A simple answer is to compute an intrinsic reward based on the statistics of each auxiliary learner, and use reinforcement learning to maximize that intrinsic reward. Unfortunately, implementing this simple idea has proven difficult, and thus has been the focus of decades of study. It remains unclear which of the many possible measures of learning would work well in a parallel learning setting where environmental reward is extremely sparse or absent. In this paper, we investigate and compare different intrinsic reward mechanisms in a new bandit-like parallel-learning testbed. We discuss the interaction between reward and prediction learners and highlight the importance of introspective prediction learners: those that increase their rate of learning when progress is possible, and decrease when it is not. We provide a comprehensive empirical comparison of 14 different rewards, including well-known ideas from reinforcement learning and active learning. Our results highlight a simple but seemingly powerful principle: intrinsic rewards based on the amount of learning can generate useful behavior, if each individual learner is introspective.},
  archivePrefix = {arXiv},
  eprint = {1906.07865},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/E9RK85MA/Linke et al. - 2020 - Adapting Behaviour via Intrinsic Reward A Survey .pdf;/home/rafael/Zotero/storage/2IDE4V54/1906.html},
  journal = {arxiv:1906.07865},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{mcculloch_logical_1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  month = dec,
  volume = {5},
  pages = {115--133},
  issn = {1522-9602},
  doi = {10.1007/BF02478259},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  journal = {The bulletin of mathematical biophysics},
  keywords = {used},
  language = {en},
  number = {4}
}

@article{mnih_playing_2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archivePrefix = {arXiv},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/KGFNZZTN/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/rafael/Zotero/storage/5VACAN87/1312.html},
  journal = {arxiv:1312.5602},
  keywords = {Computer Science - Machine Learning,used}
}

@inproceedings{pathak_curiosity-driven_2017-1,
  title = {Curiosity-{{Driven Exploration}} by {{Self}}-{{Supervised Prediction}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {2778--2787},
  address = {{Sidney, New South Wales}},
  doi = {10.5555/3305890},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  file = {/home/rafael/Zotero/storage/AKY3NBWJ/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Pr.pdf},
  keywords = {used},
  language = {en}
}

@article{pathak_self-supervised_2019,
  title = {Self-{{Supervised Exploration}} via {{Disagreement}}},
  author = {Pathak, Deepak and Gandhi, Dhiraj and Gupta, Abhinav},
  year = {2019},
  month = jun,
  abstract = {Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/},
  archivePrefix = {arXiv},
  eprint = {1906.04161},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/HDW2GRJK/Pathak et al. - 2019 - Self-Supervised Exploration via Disagreement.pdf;/home/rafael/Zotero/storage/S23G78CW/1906.html},
  journal = {arxiv:1906.04161},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning}
}

@article{rosenblatt_perceptron_1957,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  month = nov,
  volume = {65},
  pages = {386--408},
  issn = {1939-1471},
  doi = {10.1037/h0042519},
  file = {/home/rafael/Zotero/storage/L8YAKK2Z/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf},
  journal = {Psychological Review},
  keywords = {used},
  language = {en},
  number = {6}
}

@article{rumelhart_learning_1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  volume = {323},
  pages = {533--536},
  issn = {0028-0836},
  doi = {10.1038/323533a0},
  file = {/home/rafael/Zotero/storage/6I5NUCTP/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf},
  journal = {Nature},
  keywords = {used},
  language = {en},
  number = {6088}
}

@article{ryan_intrinsic_2000,
  title = {Intrinsic and {{Extrinsic Motivations}}: {{Classic Definitions}} and {{New Directions}}},
  shorttitle = {Intrinsic and {{Extrinsic Motivations}}},
  author = {Ryan, Richard M. and Deci, Edward L.},
  year = {2000},
  month = jan,
  volume = {25},
  pages = {54--67},
  issn = {0361-476X},
  doi = {10.1006/ceps.1999.1020},
  abstract = {Intrinsic and extrinsic types of motivation have been widely studied, and the distinction between them has shed important light on both developmental and educational practices. In this review we revisit the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory. Intrinsic motivation remains an important construct, reflecting the natural human propensity to learn and assimilate. However, extrinsic motivation is argued to vary considerably in its relative autonomy and thus can either reflect external control or true self-regulation. The relations of both classes of motives to basic human needs for autonomy, competence and relatedness are discussed.},
  file = {/home/rafael/Zotero/storage/B3UX45W7/Ryan and Deci - 2000 - Intrinsic and Extrinsic Motivations Classic Defin.pdf;/home/rafael/Zotero/storage/SJYTSPP7/S0361476X99910202.html},
  journal = {Contemporary Educational Psychology},
  language = {en},
  number = {1}
}

@article{schmidhuber_formal_2010,
  title = {Formal {{Theory}} of {{Creativity}}, {{Fun}}, and {{Intrinsic Motivation}}},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = sep,
  volume = {2},
  pages = {230--247},
  issn = {1943-0612},
  doi = {10.1109/TAMD.2010.2056368},
  abstract = {The simple, but general formal theory of fun and intrinsic motivation and creativity (1990-2010) is based on the concept of maximizing intrinsic reward for the active creation or discovery of novel, surprising patterns allowing for improved prediction or data compression. It generalizes the traditional field of active learning, and is related to old, but less formal ideas in aesthetics theory and developmental psychology. It has been argued that the theory explains many essential aspects of intelligence including autonomous development, science, art, music, and humor. This overview first describes theoretically optimal (but not necessarily practical) ways of implementing the basic computational principles on exploratory, intrinsically motivated agents or robots, encouraging them to provoke event sequences exhibiting previously unknown, but learnable algorithmic regularities. Emphasis is put on the importance of limited computational resources for online prediction and compression. Discrete and continuous time formulations are given. Previous practical, but nonoptimal implementations (1991, 1995, and 1997-2002) are reviewed, as well as several recent variants by others (2005-2010). A simplified typology addresses current confusion concerning the precise nature of intrinsic motivation.},
  file = {/home/rafael/Zotero/storage/L6NTMCM9/Schmidhuber - 2010 - Formal Theory of Creativity, Fun, and Intrinsic Mo.pdf;/home/rafael/Zotero/storage/EXHEZ7YW/5508364.html},
  journal = {IEEE Transactions on Autonomous Mental Development},
  keywords = {active learning,Active learning,aesthetics,aesthetics theory,art,Art,attention,cognition,Computational intelligence,creativity,data compression,Data compression,developmental psychology,event sequences,Eyes,Feedback,Fingers,formal theory of creativity,fun,humor,Intelligent robots,intrinsic motivation,intrinsic reward,limited computational resources,music,novel patterns,novelty,Pediatrics,Predictive models,Psychology,science,surprise,typology of intrinsic motivation,used},
  number = {3}
}

@article{schulman_proximal_2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archivePrefix = {arXiv},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/CTKCQUSZ/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/rafael/Zotero/storage/IMBL6B3D/1707.html},
  journal = {arXiv:1707.06347},
  keywords = {Computer Science - Machine Learning,used}
}

@inproceedings{schulman_trust_2015,
  title = {Trust {{Region Policy Optimization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  year = {2015},
  month = jun,
  volume = {37},
  pages = {1889--1897},
  issn = {1938-7228},
  doi = {10.5555/3045118},
  abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a pr...},
  chapter = {Machine Learning},
  file = {/home/rafael/Zotero/storage/V7GP35AJ/Schulman et al. - 2015 - Trust Region Policy Optimization.pdf;/home/rafael/Zotero/storage/LNCBKRJ6/schulman15.html},
  keywords = {used},
  language = {en}
}

@article{silver_mastering_2017,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self}}-{{Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archivePrefix = {arXiv},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/56DCZEI2/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf;/home/rafael/Zotero/storage/8V8ZLRZN/1712.html},
  journal = {arxiv:1712.01815},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,used}
}

@article{stadie_incentivizing_2015,
  title = {Incentivizing {{Exploration In Reinforcement Learning With Deep Predictive Models}}},
  author = {Stadie, Bradly C. and Levine, Sergey and Abbeel, Pieter},
  year = {2015},
  month = nov,
  abstract = {Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.},
  archivePrefix = {arXiv},
  eprint = {1507.00814},
  eprinttype = {arxiv},
  file = {/home/rafael/Zotero/storage/KNZZJJZ9/Stadie et al. - 2015 - Incentivizing Exploration In Reinforcement Learnin.pdf;/home/rafael/Zotero/storage/8KGPE4P7/1507.html},
  journal = {arxiv:1507.00814},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  month = may,
  edition = {Second},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  file = {/home/rafael/Zotero/storage/N7MXHSAM/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-03924-6},
  keywords = {Reinforcement learning,used},
  language = {en},
  lccn = {Q325.6 .R45 2018},
  series = {Adaptive Computation and Machine Learning Series}
}

@article{szepesvari_algorithms_2010,
  title = {Algorithms for {{Reinforcement Learning}}},
  author = {Szepesv{\'a}ri, Csaba},
  year = {2010},
  month = jan,
  volume = {4},
  pages = {1--103},
  doi = {10.2200/S00268ED1V01Y201005AIM009},
  abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective. What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book we focus on those algorithms of reinforcement learning which build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning problems, describe the core ideas, a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
  file = {/home/rafael/Zotero/storage/XX69S76Y/Szepesvári - 2010 - Algorithms for Reinforcement Learning.pdf},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  keywords = {used},
  number = {1}
}

@inproceedings{todorov_mujoco_2012,
  title = {{{MuJoCo}}: {{A}} Physics Engine for Model-Based Control},
  shorttitle = {{{MuJoCo}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  year = {2012},
  month = oct,
  pages = {5026--5033},
  issn = {2153-0866},
  doi = {10.1109/IROS.2012.6386109},
  abstract = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ API or an intuitive XML file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-defined even in the presence of contacts and equality constraints. The model can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
  file = {/home/rafael/Zotero/storage/9DAS2LAA/6386109.html},
  keywords = {12-core machine,3D humanoid,active contacts,actuator activation states,application program interfaces,built-in compiler transforms,C++ language,Computational modeling,control engineering computing,data structures,Dynamics,Engines,finite difference methods,finite differencing,Heuristic algorithms,high-level C++ API,humanoid robots,intuitive XML file format,Mathematical model,model-based control,MuJoCo,multijoint dynamics,optimal control,optimal control applications,Optimization,optimized data structure,physics engine,program compilers,recursive algorithms,runtime computation,shock absorbers,spring-dampers,tendon wrapping,velocity-stepping approach,XML}
}

@article{vezhnevets_feudal_2017,
  title = {{{FeUdal Networks}} for {{Hierarchical Reinforcement Learning}}},
  author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  year = {2017},
  month = mar,
  volume = {arxiv:1703.01161},
  abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
  file = {/home/rafael/Zotero/storage/AUKB95TC/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Lea.pdf;/home/rafael/Zotero/storage/KLNGPKD3/1703.html},
  keywords = {combining,Computer Science - Artificial Intelligence,Reward Combination}
}

@article{watkins_q-learning_1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  year = {1992},
  month = may,
  volume = {8},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  file = {/home/rafael/Zotero/storage/QAFSZ6GD/Watkins and Dayan - 1992 - Q-learning.pdf},
  journal = {Machine Learning},
  keywords = {used},
  language = {en},
  number = {3}
}

@article{white_motivation_1959,
  title = {Motivation Reconsidered: {{The}} Concept of Competence},
  shorttitle = {Motivation Reconsidered},
  author = {White, Robert W.},
  year = {1959},
  month = sep,
  volume = {66},
  pages = {297--333},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471},
  doi = {10.1037/h0040934},
  abstract = {Theories of motivation built upon primary drives cannot account for playful and exploratory behavior. The new motivational concept of "competence" is introduced indicating the biological significance of such behavior. It furthers the learning process of effective interaction with the environment. While the purpose is not known to animal or child, an intrinsic need to deal with the environment seems to exist and satisfaction ("the feeling of efficacy") is derived from it. (100 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/home/rafael/Zotero/storage/AYSDMZFA/1961-04411-001.html},
  journal = {Psychological Review},
  keywords = {Competence,Environmental Effects,Exploratory Behavior,Learning,Motivation,Recreation},
  number = {5}
}


