% Copyright (C) 2014-2020 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesettings of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{hyperref}  % Enables cross linking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists fo acronyms. This package has to be included last.
\usepackage{natbib}
\usepackage{amsfonts}

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Rafael Sterzinger} % The author name without titles.
\newcommand{\thesistitle}{Balancing Extrinsic and Intrinsic Rewards in Reinforcement Learning}
%Prediction Error based Curiosity and How to Combine Extrinsic and Intrinsic Rewards
\newcommand{\thesissubtitle}{}% The title of the thesis. The English version should be used, if it exists.
\newcommand{\p}[1]{see p. #1}

% Set PDF document properties
\hypersetup{
pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
linkbordercolor = {Melon},                % The color of the borders of boxes around crosslinks (optional).
pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
pdfsubject      = {Bachelor's thesis on \thesistitle; Written by \authorname in the context of TARO/INSO research group},         % The document's subject in the document properties (optional).
pdfkeywords     = {deep reinforcement learning, sparse rewarding environments, prediction error based curiosity, balancing extrinsic and intrinsic rewards} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph identation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{}{male}
\setauthorextra
\setadvisor{Ao.Univ.Prof. Dipl.-Ing. Dr.techn.}{Thomas Grechenig}{}{male}

% For bachelor and master theses:
\setfirstassistant{Dipl.-Ing.}{Michael Ressmann}{}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
%\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{11778282}
\setdate{10}{08}{2020} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{\thesistitle} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
\setsubtitle{\thesissubtitle}{\thesissubtitle} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor / phd-school.
% Bachelor:
\setthesis{bachelor}
%
% Master:
%\setthesis{master}
%\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.
%
% Doctor at the PhD School
%\setthesis{phd-school} % Deactivate non-English title pages (see below)

% For bachelor and master:
\setcurriculum{Business Informatics }{Wirtschaftsinformatik} % Sets the English and German name of the curriculum.

% For dissertations at the PhD School:
%\setfirstreviewerdata{Affiliation, Country}
%\setsecondreviewerdata{Affiliation, Country}


\newacronym{im}{IM}{Intrinsic Motivation}
\newacronym{em}{EM}{Extrinsic Motivation}
\newacronym{rl}{RL}{Reinforcement Learning}
\newacronym{mdp}{MDP}{Markov Decision Process}
\newacronym{ml}{ML}{Machine Learning}
\newacronym{mlp}{MLP}{Multi Layer Perceptron}
\newacronym{nn}{NN}{Neural Network}
\newacronym{dp}{DP}{Dynamic Programming}
\newacronym{dl}{DL}{Deep Learning}
\newacronym{drl}{DRL}{Deep Reinforcement Learning}
\newacronym{mc}{MC}{Monte-Carlo}
\newacronym{td}{TD}{Temporal-Difference}
\newacronym{ppo}{PPO}{Proximal Policy Optimization}
\newacronym{trpo}{TRPO}{Trust Region Policy Optimization}
\newacronym{relu}{ReLU}{Rectified Linear Unit}


\begin{document}

    \frontmatter % Switches to roman numbering.
%  The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

    \addtitlepage{naustrian} % German title page (not for dissertations at the PhD School).
    \addtitlepage{english} % English title page.
    \addinsotitlepage{naustrian}
    \addstatementpage

    \begin{danksagung*}

Ich möchte diese Arbeit meine beiden Eltern widmen, welche mich während meines Studiums nicht nur finanzielle, sondern auch moralisch unterstützt haben. Insbesondere während der Verfassung dieser Bachelorarbeit möchte ich mich dafür bedanken, dass ihr immer für mich da wart und mich in meinem Bestreben ermutigt habt.

Ebenfalls ein großes Dankeschön möchte ich an meinen Betreuer Michael Ressmann ausprechen, welcher mir während der Planung und Durchführung der Arbeit den nötigen Freiraum gegeben hat, zeitgleich aber auch darauf geachtet hat, dass ich mich in meinem Vorhaben nicht übernehme. Außerdem möchte ich mich bei ihm für sein gutes Feedback sowie für die rasche Benotung meiner Bachelorarbeit bedanken, welcher aufgrund zeitlicher Beschränkungen meinerseits notwendig war.

Außerdem möchte ich mich bei meinen Freunden bedanken, insbesondere bei meinen langjährigen Freunden, Stefan Zeller und Vinzenz Schicho, welche mich in diesen intensiven Wochen begleitet und mir gut zugesprochen haben.
Zu guter Letzt
% wöchentlichen treffen um energie zu tanken für weiteres verfassen
%TODO danke für korrektur lesen
	        \end{danksagung*}

    \begin{acknowledgements*}
	    I would like to dedicate this work to my parents, who supported me not only financially but also morally during my studies. In particular while writing this Bachelor's thesis, I would like to thank both of you for always being there for me and for encouraging me in my endeavors.

	    I would also like to express a big thank you to my supervisor Michael Ressmann, who gave me the necessary freedom during the planning and execution of this Bachelor's thesis, but at the same time also made sure that I did not take on too much during my project. I would also like to thank him for his good feedback and for the quick grading of my thesis, which was necessary due to time constraints on my part.

	    Also, I would like to thank my friends, especially by my long-standing friends, Stefan Zeller and Vinzenz Schicho, who accompanied me during these intensive weeks and cheered me up.
	    Last but not least, 
%TODO danke für korrektur lesen

    \end{acknowledgements*}

    \begin{kurzfassung}
    \end{kurzfassung}

    \begin{abstract}
    \end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
    \selectlanguage{english}

% Add a table of contents (toc).
    \tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
    \mainmatter


    \chapter{Introduction}

    %TODO write introduction


    \section{Problem Description}\label{sec:problem-description}
    In \gls{rl}, algorithms mainly depend on carefully designed extrinsic reward functions which are comparable with feedback to an agent's behaviour.
    In order to teach an agent a certain desired behaviour, it has to optimize this reward function by trial-and-error.
    This traditional approach yielded multiple astonishing results over the last decade.
    Task which award rewards densely, i.e.\ almost after every performed action, are common for these results.
    In opposite to dense rewards, sparse rewards are scattered in the environment which poses the problem that they occur too rarely in order for an agent to pick up the needed skills.
    An example for environments with sparse rewards are tasks which depend heavily on exploration, e.g. \textit{Montezumas's revenge}.
    \\\\
    An approach to tackle these problems is curiosity, an organism's \gls{im} to spontaneously explore its environment.
    \gls{im} allows an agent to incrementally learn valuable skills independently of its given task by an intrinsic and therefore a more general reward function.
    In general, the concept of intrinsic and extrinsic evolves around the question on why an agent performed a certain action in a given state.
    Since \gls{im} became an important part of \gls{rl}, the following challenges were crystallized out by~\cite[\p{6}]{aubret_survey_2019}:

    \begin{itemize}
        \item \textbf{Sparse rewards:} The agent never reaches a reward signal in case of sparse rewards.
        \item \textbf{State representation:} The agent does not manage to learn a representation of its observations with independent features or meaningful distance metrics.
        \item \textbf{Building option:} The agent is unable to learn abstract high-level decisions independently from the task.
        \item \textbf{Learning a curriculum:} The agent hardly defines a curriculum among its available goals without expert knowledge.
    \end{itemize}

    Besides a general introduction to \gls{im} and its challenges in \gls{rl}, this Bachelor's thesis has its focus on knowledge acquisition through exploration.
    Knowledge acquisition is described as the agents motivation to find new knowledge about its environment, meaning that it is interested in things it can or cannot control, the function of the world, discovering new areas, or understanding the proximity.
    Concerning exploration, one approach is error prediction which is the agents difficulty to predict the state following a state-action-tuple.
    This idea is heavily explored by~\cite{burda_large-scale_2018-1} and its prior work of~\cite{pathak_curiosity-driven_2017-1}, both of which are core references of this thesis.
    \\\\
    Building upon the findings of~\citeauthor{burda_large-scale_2018-1}, this Bachelor's thesis eyes on answering the question on how to optimally combine intrinsic and extrinsic rewards in order to maximize an agents score, primarily in sparse but also in dense environments.
    Furthermore, it will evaluate if a balanced reward combination tackles the noisy-TV problem, proposed by~\cite{schmidhuber_formal_2010-1}.
    The noisy-TV problem, also known as the white-noise problem, is an algorithms inability to handle the local stochasticity of the environment: "\ldots random noise in a 3D environment attracts the agent; it will passively watch the noise since it will not be able to predict the next state."\cite[\p{10}]{aubret_survey_2019}
    \\\\
    The domain of the given problem description is \gls{rl} in \gls{rl} with a focus on the model of knowledge acquisition via exploration.


    \section{Expected Results}\label{sec:expected-results}
    As aforementioned, the goal pursued by this Bachelor's thesis is to build upon the results of~\citeauthor{burda_large-scale_2018-1} and to pursue their posed question on how to optimally combine intrinsic ($r_{int}$) and extrinsic ($r_{ext}$) rewards.
    In order to do so, coefficients $\alpha$ and $\beta$ have to be selected accordingly.
    \[r=\alpha r_{int} + \beta r_{ext}\]
    From that, an optimal solution to this equation maximizes the agent's overall reward when testing.
    After a re-implementation of the proposed algorithms, the benchmarks are expected to at least match the mean rewards mentioned in the \textit{Additional Results} section from the underlying paper.
    In this section, the authors explored the performance of combined rewards on five different Atari games and already showed that the combination of rewards yield a higher mean reward.
    Therefore, an exhaustive hyper-parameter tuning should certainly improve these results.
    \\\\
    However, this raises the question on how the algorithm performs in comparison to comparable approaches such as the paper authored by~\cite{kim_emi_2019}.
    In some of the previously mentioned hard games, e.g. \textit{Frostbite}, the authors achieved only a slightly higher mean score which might be beatable by combining rewards.
    Optionally, it would be interesting to observe the performance in environments with dense rewards too since there might be an improvement to see as well.
    \\\\
    In the \textit{Discussion} section,~\citeauthor{burda_large-scale_2018-1} mention the limitations of prediction error based curiosity which was earlier introduced with the so called noisy-TV problem.
    Given this problem, the question is raised on whether or not it is possible to tackle the agent's distraction through curiosity with \gls{em}.
    This question will be pursued during the evaluation process with the posed hypothesized that it is possible to overcome an agent's distraction and the assumption that \gls{em} has a higher impact on the distraction in a densely rewarding environment than in a sparse one.
    \\\\
    With the given prospect to beat state of the art approaches to environments with sparse rewards and evaluate the amount of extrinsic reward needed to put a distracted agent back on track, it is the opportunity to pursue important constructs, which reflect the natural human propensity to learn and assimilate, in the domain of computer science that motivated this Bachelor's thesis.


    \section{Methodological Approach}\label{sec:methodological-approach}
    In order to answer the question on how to optimally combine intrinsic and extrinsic rewards, the thesis builds upon the released source code and environments from~\cite{burda_large-scale_2018-1} and~\cite{pathak_curiosity-driven_2017-1} which can be observed on these websites. \footnote{\url{https://pathak22.github.io/large-scale-curiosity}}\footnote{\url{https://pathak22.github.io/noreward-rl/}}
    Therefore, this Bachelor's thesis follows a programming approach which relies on the same tools used by the preceding authors, since it guarantees optimal reproducibility of their results.
    In an overview, these tools are the \textit{Python}\footnote{\url{https://www.python.org/}} programming language, the \gls{ml} platform \textit{TensorFlow}\footnote{\url{https://www.tensorflow.org/}}, and lastly the \textit{OpenAI Gym}\footnote{\url{https://gym.openai.com/}} which is a toolkit for developing and comparing \gls{rl} algorithms.
    \\\\
    The development progress is structured into the following five mile stones:

    \begin{enumerate}
        \item Setting-up of the environment, including the installation of the \textit{CUDA}\footnote{\url{https://developer.nvidia.com/cuda-toolkit}} toolkit and the \textit{cuDNN}\footnote{\url{https://developer.nvidia.com/cudnn}]} library
        \item Implementing a Proximal Policy Optimization (PPO) algorithm according to~\cite{schulman_proximal_2017}, using extrinsic rewards
        \item Adding an Intrinsic Curiosity Module, published by~\cite{pathak_curiosity-driven_2017-1}, in order to allow for intrinsic rewards
        \item Tuning of the coefficients $\alpha,\beta$ and comparison to state of the art approaches
        \item Recreation of the synthetic generated noisy-TV problem, proposed by~\cite{burda_large-scale_2018-1}
    \end{enumerate}

    During this incremental process, an emphasise is laid on two out of five sparse Atari games, categorized by~\cite{bellemare_unifying_2016} and picked by~\citeauthor{burda_large-scale_2018-1}.
    The initial two games are \textit{Montezumas's revenge} and \textit{Freeway}, but with the option to add the remaining ones or games with dense rewards in the future.
    \\\\
    % Eventually add VAE
    Regarding mile stone three, \textit{Random Features} and \textit{Inverse Dynamics Features} as introduced by~\citeauthor{burda_large-scale_2018-1} are primarily the focus.
    Concerning mile stone four and the comparison to state of the art approaches, the~\nameref{sec:state-of-the-art} section of this thesis allows for a current in-depth overview.
    Additionally to the aforementioned practical part, the theoretical aspect of this Bachelor's thesis will be covered by an exhaustive literature research with an emphasize on directly comparable approaches, i.e. prediction error based curiosity~\citep{burda_large-scale_2018-1}.


    \section{State of the Art}\label{sec:state-of-the-art}


    \section{Thesis Outline}\label{sec:thesis-outline}


    \glsresetall

    \chapter{Reinforcement Learning Background}\label{ch:reinforcement-learning-background}

    %TODO write introduction


    \section{Reinforcement Learning Problem}\label{sec:reinforcement-learning-problem}
    Reinforcement Learning, the science of decision making, is about learning the consequences of performed actions and using this knowledge to maximize a numeric \textit{reward signal} which should direct the search for a given objective~\citep{sutton_reinforcement_2018}.
    \gls{rl} problems are tackled by a learner or rather an \textit{agent} that performs actions in an \textit{environment} in a trial and error paradigm.
    The agent has no prior knowledge of the consequences of the possible actions but has to explore which action to take in a certain situation to maximize the amount of obtainable rewards.
    An additional difficulty is added if the reward for an action is delayed because the agent has to understand which of the executed actions actually led to the desirable reward it received far ahead in the future.
    Lastly, a proper weighting between exploration and exploitation has to be developed as the agent must exploit actions with known consequences to perform well but also explore new ones in order to improve~\citep{kaelbling_reinforcement_1996}.

    In addition to the aforementioned environment, agent, and reward signal, \gls{rl} consists of further subelements since it would be desirable e.g. not only to know the immediate reward but also to gain a measurement for all possible future rewards from a given state onward.
    For this purpose, a \textit{value function} is used.
    It acts as a long-term indicator that describes the attractiveness of a state an agent might result in after interacting with its environment.
    This measurement is interesting because there could be an action which leads to a high immediate reward but results in an undesirable state with no future rewards.
    Furthermore, the agent should pick up a \textit{policy} that consists of state-action tuples which should be refined and adjusted depending on the received rewards.
    Lastly, the agent may want to build a \textit{model} which estimates the environment's behaviour, in order to plan its actions in advance.
    This final addition divides the \gls{rl} problem spectrum into two parts: model-free and model-based approaches~\citep{sutton_reinforcement_2018}.


    In opposite to other known \gls{ml} disciplines such as \textit{supervised learning}, there is no supervisor in \gls{rl} that explicitly tells the agent which action would be correct in a given state and because of that, the agent has to come up with its own policy based on the repeated feedback it receives from the immediate reward and the following state~\citep{kaelbling_reinforcement_1996}.
    This spares the necessity of creating a training dataset with corresponding state-action tuples beforehand and thus \gls{rl} is the preferred choice in cases where an agent interacts with and influences its environment.
    Furthermore, teaching based on supervision would also mean that an agent will be limited to the labelling ability of the supervisor which is another reason why it must be able to learn from its own experiences~\citep{silver_lecture_2015-2}.
    Since \gls{rl} is quite distinct from supervised learning and does not rely on labelled examples, it is sometimes misclassified as \textit{unsupervised learning} which has the purpose to find structure in unlabelled data.
    However, this is not applicable for \gls{rl} as the agent has to maximize a reward function and not to find structure in data.
    Therefore, \citeauthor{sutton_reinforcement_2018} classified \gls{rl}  as a third subfield of \gls{ml}.


    \section{Markov Decision Processes}
    In order to formalize the \gls{rl} problem, the idea and notation of a \gls{mdp} is used to notate the sequential interactions between an agent and its environment.
    At every discrete time step, $t = 0,1,2,3, \ldots \in \mathbb{N}$, the agent observes the current state, $S_t \in \mathit{S}$, of the environment and selects an available action, $A_t \in \mathit{A}(s)$~\eqref{fig:rl_problem}.
    %Add note concerning R_t wich is nowadays used in DRL
    Once the agent has selected an action, it obtains a scalar reward $R_{t+1} \in \mathit{R} \subset \mathbb{R}$ and depending on the action the environment transforms into a subsequent state $S_{t+1}$.
    By means of continuous interactions between the agent and the environment a trajectory of the form $S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\ldots$ is created.

    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{figures/rl_problem.png}
        \caption[The \gls{rl} framework formalized as a \gls{mdp}]{The \gls{rl} framework formalized as a \gls{mdp}\protect\footnotemark}
        \label{fig:rl_problem}
    \end{figure}

    \footnotetext{\cite[\p{48}]{sutton_reinforcement_2018}}

    In a finite-horizon, the agent has to plan only a fixed number of time-steps ahead and the sets $\mathit{S},\mathit{A}\text{, and }\mathit{R}$ are limited~\citep{kaelbling_reinforcement_1996,sutton_reinforcement_2018}.
    Furthermore, the possibility that the values $s'\in \mathit{S}$ and $r \in \mathit{R}$ may occur at a time step $t+1$, can be described with a probability~\eqref{eq:distribution} solely depending on the action $a \in \mathit{A}(s)$ taken in a preceding state $s \in \mathit{S}$.

    \begin{equation}
        p(s',r|s,a) = P\{S_{t+1}=s', R_{t+1}=r | S_t=s, A_t=a\}\label{eq:distribution}
    \end{equation}

    Since $p$ describes a probability, the sum over all possible combinations of $s$ and $a$ must be equal to 1.
    Moreover, the individual probabilities of $p$ describe the dynamics of a \gls{mdp}.
    Observing this notation shows that the state $s$ must contain all the necessary information and must not depend on states preceding $s$.
    If this is the case, the state is called to have the Markov property~\eqref{eq:markov_property}~\citep{francois-lavet_introduction_2018}.

    \begin{equation}
        P\{S_{t+1}|S_t,A_t\} = P\{S_{t+1}|S_1,A_1,\ldots,S_t,A_t\} \label{eq:markov_property}
    \end{equation}

    With the notation of $p$, properties of the \gls{mdp} like the state-transition probabilities~\eqref{eq:transition} or the expected value of rewards~\eqref{eq:expected_reward} with given state-action tuples can be computed.

    \begin{equation}
        p(s'|s,a) = P\{S_{t+1}=s'| S_t=s, A_t=a\} = \sum_{r \in \mathit{R}} p(s',r | s,a) \label{eq:transition}
    \end{equation}

    \begin{equation}
        r(s',s,a) = \mathbb{E}[R_{t+1} | S_t=s, A_t=a] = \sum_{r\in \mathit{R}} r \sum_{s' \in \mathit{S}} p(s',r | s,a) \label{eq:expected_reward}
    \end{equation}

    As mentioned in the~\nameref{sec:reinforcement-learning-problem}, rewards at each time-step $R_t$ are used to formalize a goal which the agent should pursue.
    This is possible due to the reward hypothesis which states that every goal one can possibly think of, can be formalized by means of a numeric reward signal and be achieved by maximizing the cumulative long-term rewards~\citep{sutton_reinforcement_2018}.

    Mathematically, the overall reward is denoted as the sum over all rewards~\eqref{eq:sum_of_rewards}, with $T$ as the final step.

    \begin{equation}
        G_t=R_{t+1} + R_{t+2}+ \ldots + R_{T}  =\sum_{t=1}^{T} R_{t}\label{eq:sum_of_rewards}
    \end{equation}

    However, this notation is only applicable if an environment has an identifiable terminal state.
    When this state is reached, it breaks for instance a game sequence into multiple ones, where each one is called an episode.
    No matter how the agent reached the final state, the environment will reset and start over with an default initial state.

    Sometimes, this separation of episodes might not be as clear and the limit of the sequence is infinite.
    This is called a continuing task, in contrast to episodic tasks where $T$ is finite.
    Therefore, if the agent acts in an environment with a continuing task, optimizing the equation~\eqref{eq:sum_of_rewards} might not be finite.
    This issue is solved by introducing a discounting rate $\gamma$, $0 \leq \gamma \leq 1$~\citep{sutton_reinforcement_2018} which avoids infinite rewards.
    Given this discounting rate, the agent now tries to maximize the discounted sum of cumulative long-term rewards~\eqref{eq:discounted_reward}.

    \begin{equation}
        G_t = R_{t+1} + \gamma R_{t+2}+ \gamma^2 R_{t+3} + \ldots = R_{t+1} + \gamma G_{t+1} = \sum_{k=1}^{\infty} \gamma^k R_{t+k}\label{eq:discounted_reward}
    \end{equation}

    With $\gamma$ as newly introduced hyperparameter, the weighting of the rewards can be adjusted according to the given task.
    A low $\gamma$ means that the agent values more the immediate reward and rewards in the near future.
    Whereas, a high $\gamma$ forces the agent to consider long-term future rewards as well and thus the agent is more far-sighted.


    \section{Functions to Improve the Policy}\label{sec:functions-to-improve-the-policy}
    As mentioned in the~\nameref{sec:reinforcement-learning-problem} section, the policy forms an important component of the \gls{rl} framework and is usually denoted as $\pi$.
    By following a policy, the agent's probability to select a specific action at time-step $t$ is $\pi(a|s) = P\{A_t=a|S_t=s\}$ and therefore $\pi$ describes a probability distribution over actions given states~\citep{silver_lecture_2015-1}.
    Additionally, the aforementioned value function component is dependent on the policy, since $\pi$ defines which action-state tuples are considered from the current state onwards.
    Formally, the state-value function for an \gls{mdp} is denoted by $v_\pi(s)$ which stands for the expected future rewards when following the policy, starting in state $s$~\citep{sutton_reinforcement_2018}.

    \begin{equation}
        v_\pi(s) = \mathbb{E}_\pi[G_t|S_t = s] = \mathbb{E}_\pi \Bigg[\sum_{k=1}^{\infty} \gamma^k R_{t+k} \bigg| S_t = s \Bigg]\label{eq:value_function}
    \end{equation}

    In a similar way, the action-value function $q_\pi(s,a)$ of a \gls{mdp} can be defined.
    This calculates the expected future reward when the agent takes action $a$ in state $s$ and then follows the policy $\pi$.

    \begin{equation}
        q_\pi(s,a) = \mathbb{E}_\pi[G_t|S_t = s, A_t = a] = \mathbb{E}_\pi \Bigg[\sum_{k=1}^{\infty} \gamma^k R_{t+k} \bigg| S_t = s | A_t = a \Bigg]\label{eq:quality_function}
    \end{equation}

    The policy is improved by adjusting the value functions $v_\pi$ and $q_\pi$ through the agent's obtained experience.
    Since \gls{rl} aims to solve a given objective by maximizing rewards, it implicitly goes along with optimizing the agent's policy.
    If a policy $\pi$ achieves a higher value than a different policy $\pi'$, which also implies that $v_\pi(s) \geq v_{\pi'}(s), \forall s \in \mathit{S}$, then the two policies can be ordered partially, with $\pi \geq \pi'$.
    However, there can be multiple policies performing equally well~\citep{sutton_reinforcement_2018} and for any \gls{mdp} the following theorem holds true~\citep[\p{43}]{silver_lecture_2015-1}:

    \begin{itemize}
        \item There exists an optimal policy $\pi_*$ that is better than or equal to all other policies, $\pi_* \geq \pi, \forall\pi$
        \item All optimal policies achieve the optimal [state-]value function, $v_{\pi_*}(s) = v_*(s)$
        \item All optimal policies achieve the optimal action-value function, $q_{\pi_*}(s,a) = q_*(s,a)$
    \end{itemize}

    A very simple optimal policy for a given state can be derived through the action-value function $q_*(s,a)$, simply by selecting the action with the maximum value.

    \begin{equation}
        \pi_*(a|s) =
        \begin{cases}
            1 \text{ if } a =  \underset{a \in \mathit{A}}{\text{argmax}}\ q_*(s,a),\\
            0 \text{ otherwise }
        \end{cases}
    \end{equation}

    The optimal value functions are given by the following equations:

    \begin{equation}
        \begin{aligned}[t]
            v_*(s) &= \underset{\pi}{\text{max }}v_\pi(s), \forall s \in \mathit{S}, \\
            q_*(s,a) &= \underset{\pi}{\text{max }}q_\pi(s,a), \forall s \in \mathit{S} \land \forall a \in \mathit{A}\label{eq:optimal}
        \end{aligned}
    \end{equation}


    \section{Dynamic Programming}

    The value function, introduced in the previous section~\nameref{sec:functions-to-improve-the-policy}, has a similar recursive property as the equation for the discounted expected reward~\eqref{eq:discounted_reward}.
    Therefore, the value function can be expressed in an equal manner by decomposing it into the discounted descendant states $\gamma v_\pi(s')$ and the immediate reward $r$:

    \begin{equation}
        \begin{aligned}[t]
            v_\pi(s) &= \mathbb{E}_\pi[G_t|S_t=s] = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s] \\
            &  =  \sum_{a} \pi(a|s) \sum_{s'}\sum_{r} p(s',r|s,a) \bigg[r + \gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1} = s'] \bigg] \\
            &  =  \sum_{a} \pi(a|s) \sum_{s'}\sum_{r} p(s',r|s,a) \bigg[r + \gamma v_{\pi}(s') \bigg], \forall s \in \mathit{S} \\
            &  =  \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t=s]
            \label{eq:bellman_equation}
        \end{aligned}
    \end{equation}

    The above mentioned equation~\eqref{eq:bellman_equation} is known as the Bellman equation for $v_\pi$~\citep{sutton_reinforcement_2018} which can be directly solved due to its linearity.
    However, this calculation is only feasible for small \glspl{mdp} since its complexity is $O(n^3)$ for $n$ states~\citep{silver_lecture_2015-1}.

    For larger \glspl{mdp} exist multiple iterative solutions as for example:
    \begin{itemize}
        \item \gls{dp}
        \item \gls{mc} Evaluation
        \item \gls{td} Learning
    \end{itemize}

    The general idea of \gls{dp}, which was developed by \citeauthor{bellman_theory_1954}, is to make use of the property of recursion to simplify the complexity of calculations by storing interim results.
    In that sense, it is a method that aims at solving complex problems by breaking them down into manageable subproblems.

    All of the three above mentioned approaches are mainly used for two aspects in \gls{rl}: predicting the value function $v_\pi$ (planning) and calculating the optimal value function $v_{\pi_*}$ (controlling).
    As for \gls{dp}, an iterative process is is used.
    This process is an interplay between the Bellman equation~\eqref{eq:bellman_equation}, used to calculate the value function for a given policy, and the equations mentioned in~\eqref{eq:optimal} which update the policy~\citep{silver_lecture_2015,sutton_reinforcement_2018}.


    \section{Temporal-Difference Methods}

    %Write introduction for td using page 119 of sutton

    \subsection{Temporal Difference Prediction}
    Besides \gls{dp} and \gls{mc} evaluation, \gls{td} prediction is among one of the three mentioned techniques, combining the advantages of the other two for value function predictions.
    Firstly, it uses the benefit of bootstrapping offered by \gls{dp}~\citep{szepesvari_algorithms_2010}.
    That is the usage of predictions of the value function as the target during the learning process.
    Secondly, \gls{td} prediction directly learns from raw experience in a similar way to \gls{mc} evaluation and thus it is model-free.
    However, it only awaits the reward of one time step to form a target for the value function instead of the every-visit \gls{mc} method which awaits the fully known return for the target.

    \begin{equation}
        \begin{aligned}[t]
            \text{\gls{mc} method: } & V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]\\
            \text{\gls{td} method: } & V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1})-V(S_t)]\\
        \end{aligned}\label{eq:td_versus_mc}
    \end{equation}

    The parameter $\alpha$ is constant which defines the step-size of each adjustment to the value function.
    The two equations express the above mentioned differences certainly well:

    \begin{quote}
        Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need to wait only until the next time step.
        At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$.

        \hfill~\cite[\p{120}]{sutton_reinforcement_2018}
    \end{quote}

    The \gls{td} method is also known as $\text{TD}(0)$ that refers to the general case $\text{TD}(\lambda)$, a method that unifies \gls{td} and \gls{mc}.

    \subsection{SARSA (On-Policy)}\label{subsec:sarsanullon-policynull}

    SARSA builds upon the idea of \gls{td} but aims to find an optimal value function and thus solve the control problem.
    In a similar manner to \gls{td}, SARSA aims to estimate the action-value function $q_\pi(s,a)$, given a current policy $\pi$.
    Since it only focuses on one policy at a time, it is also known as an on-policy algorithm.
    The action-value function is optimized by means of the following calculation:

    \begin{equation}
        Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_{t},A_{t}) ]
    \end{equation}

    At each transition between the state-action tuple, the values $S_{t},A_{t},R_{t+1},S_{t+1},A_{t+1}$ are included in the optimization process, eponymous for SARSA~\citep{sutton_reinforcement_2018}.
    Under the assumption that every state-action tuple is visited infinite times and that new policies are created greedily with respect to the current action-value function, SARSA is guaranteed to converge to the optimal action-value value function and thus to the optimal policy.


    %TODO Additionally add pseudo code of sarsa

    \subsection{Q-Learning (Off-Policy)}\label{subsec:q-learningnulloff-policynull}

    Another control algorithm which is build upon the idea of \gls{td} is Q-learning which aims to directly estimate the optimal action-value function $q_*$.
    As denoted by \citeauthor{watkins_q-learning_1992}, the following calculation is used to update the action-value function $Q$.

    \begin{equation}
        Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma \underset{a}{\text{max}} Q(S_{t+1},a) - Q(S_{t},A_{t}) ]
    \end{equation}

    As it does not optimize for the current policy but directly for the optimal one, Q-learning is considered an off-policy method and the policies must no be interchanged~\citep{szepesvari_algorithms_2010}.

    %TODO Additionally add pseudo code of q


    \section{Policy Gradient Methods}


    %TODO make separate chapter

    \glsresetall

    \chapter{Deep Reinforcement Learning}\label{ch:deep-reinforcement-learning}


    In the preceding chapter about the~\nameref{ch:reinforcement-learning-background}, multiple solutions to the \gls{rl} problem were presented.
    However, approaches as SARSA~\eqref{subsec:sarsanullon-policynull} or Q-learning~\eqref{subsec:q-learningnulloff-policynull} only perform well if each state of $\mathit{S}$ is visited countless times in order for these methods to estimate the optimal policy $\pi$.
    In the case of video games where states are often represented as the current frame of the game, the dimension of the state-space $\mathit{S}$ is intangible which is also known as the curse of dimensionality~\citep{goodfellow_deep_2016}.
    That is because each frame consists of thousands of pixels and each of this pixels can accept various values.
    To solve this issue, \gls{dl} techniques which emerged from \glspl{nn} are applied to \gls{rl} due to there capability to estimate very complex functions such as the value function for a high dimensional state-space.
    By combining these techniques the field of \gls{drl} emerged which allows for superhuman-like problem solving capabilities~\citep{francois-lavet_introduction_2018}.

    Instead of directly going through adapted traditional \gls{rl} approaches, this chapter starts with an introduction to \glspl{nn} and \gls{dl}.
    Afterwards, the chapter contains an overview on traditional \gls{drl} approaches which is finally concluded with one specific optimization method,~\gls{ppo}~\eqref{sec:proximal-policy-optimization}.
    A focus is laid on this approach because \gls{ppo} represent the algorithm which is used in the work of \citeauthor{burda_large-scale_2018-1}, the core literature of this thesis.

    %TODO write trpo
%    Instead of directly going through adapted traditional \gls{rl} approaches, this chapter starts with an introduction to \glspl{nn} and \gls{dl}.
%    Afterwards, the chapter contains an overview on traditional \gls{drl} approaches which is finally concluded with two specific optimizations methods,~\gls{trpo}~\eqref{sec:trust-region-policy-optimization} and~\gls{ppo}~\eqref{sec:proximal-policy-optimization}.
%    A focus is laid on these two approaches because \gls{ppo} represent the algorithm which is used in the work of \citeauthor{burda_large-scale_2018-1} and \gls{trpo} as \gls{ppo} is based on this idea.
 

    \section{The Basic Architecture of Neural Networks}\label{subsec:the-basic-architecture-of-neural-networks}


    As many ideas presented in this thesis, \gls{nn} are also inspired from nature as its structure is a resemblance of the human brain.
    The name already indicates that a \gls{nn} is made of \textit{neurons} which are connected with each other by \textit{dendrites} and \textit{axons}.
    Additional connections between axons and dendrites, called \textit{synapses}, define how important a transferred signal between them is.
    Similar to a human synapse, the strength of an artificial one is changed via a learning process which refines the weighting~\citep{aggarwal_neural_2018}.
    This idea of representation was initialized by \citeauthor{mcculloch_logical_1943} who started with simple model which was later on adapted and advanced by \citeauthor{rosenblatt_perceptron_1957}~\citep{goodfellow_deep_2016}.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.75\textwidth]{figures/neuron_model.png}
        \caption[From a biological neuron to a mathematical model: an artificial neural network in its simplest form]{From a biological neuron to a mathematical model in its simplest form: an artificial neural network\protect\footnotemark}
        \label{fig:neuron_model}
    \end{figure}

    \footnotetext{\url{https://cs231n.github.io/neural-networks-1/}}

    In its simplest form, this \gls{nn} is called a \textit{perceptron} or \textit{feed-forward network}, with one input layer and one output node, see figure~\ref{fig:neuron_model}.
    In this form, the \gls{nn} allows for a generalized variation of a linear function.
    A general perceptron is of the form $(\bar{\mathbf{X}},y)$, where $\bar{\mathbf{X}}=[x_0,x_1,\ldots,x_d]$ are the \textit{input variables} in vector notation and $y \in \mathbb{R}$  represents the \textit{observed value}.
    The observed value represents a numerical scalar used for a regression problem, e.g. predicting the price of a house give some numerical input variables such as the amount of rooms and square meter.
    In the case of classification, this perceptron can be used for binary classification where the observed value is $y$ $\in [-1,+1]$.
    For instance, this can be used to predict whether or not a credit-card transaction was fraudulent given the amount and frequency of the transaction.
    Given the input values $\bar{\mathbf{X}}$ and the weights $\bar{\mathbf{W}}=[w_0,w_1,\ldots,w_d]$, a result of a linear function is calculated by the following a prediction.

    \begin{equation}
        \bar{\mathbf{X}}\cdot\bar{\mathbf{W}}=\sum_{j=0}^{d}x_j*w_j\label{eq:feed_forward_equation}
    \end{equation}

    Lastly, a prediction $\hat{y}$ for $y$, which based on the equation~\eqref{eq:feed_forward_equation}, is performed by activating the result using an \textit{activation function}.
    In the case of classification the \textit{sign activation function} is used to map the prediction $\hat{y}$ to the previously specified range of $[-1,1]$.
    After the output value has been activated, an error between the prediction and the actual outcome can be calculated, $E(\bar{\mathbf{X}}) = y - \hat{y}$~\citep{aggarwal_neural_2018}.
    If the error $E(\bar{\mathbf{X}})$ is nonzero, the weights must be adapted in order to perform a closer prediction next time.
    By optimizing the error function or \textit{loss function}, the classification error is reduced.
    The optimization process is also known as \textit{gradient descent} which updates the weights according to the negative direction of the function's gradient with respect to each input and is formally described by the following notation.

    \begin{equation}
        \bar{\mathbf{W}} \Leftarrow \bar{\mathbf{W}} + \alpha E(\bar{\mathbf{X}})\bar{\mathbf{X}}\label{eq:weight_adjusting}
    \end{equation}

    The parameter $\alpha$ is the \textit{learning rate} which defines how large the updates to the weights should be.
    A large value of $\alpha$ allows for faster convergence to a local optima with the accompanying risk of never reaching one at all.
    On the other hand, a small value steadily approaches the optima but takes longer.


    As the perceptron represents a linear function, it must not only learn the slope but also the intercept, the invariant part.
    The intercept value is also called bias and is especially necessary in scenarios where all the features are centered around the mean and the values of the target class are not centered around the origin~\citep{aggarwal_neural_2018}.
    %p. 6
    To solve this issue, the bias variable is introduced to the perceptron as an additional neuron and an extra weight which is adjusted during the error optimization process.
    This additional neuron solely has the purpose to feed the scalar 1 to the output node and by refining the weight in between, the intercept value is learned.
    By incorporating the bias neuron, the following equation emerges.

    \begin{equation}
        \bar{\mathbf{X}}\cdot\bar{\mathbf{W}} + b=\sum_{j=0}^{d}x_j*w_j + b\label{eq:linear_function_equation}
    \end{equation}

    \subsection{Activation Functions}
    In general, activation functions play an important role in \glspl{nn} as they define the output values of neurons.
    However, they are especially significant for \glspl{mlp} as they determine the modeling power of the network.
    By using nonlinear functions, more sophisticated compositions can be created as they are not reducible to simple perceptrons~\citep{aggarwal_neural_2018}.
    %p. 13

    Besides the aforementioned sign function, other activation functions are used and a selection of them can be observed in figure~\ref{fig:activation_functions}.
    As mentioned in subsection \ref{subsec:the-basic-architecture-of-neural-networks}, the sign function allows for binary classification of class labels.
    To be able to also evaluate the certitude of a \gls{nn}, the \textit{sigmoid activation function} is used instead.
    Using this function allows to map the pre-activated value to a range between $[0,1]$, allowing to estimate the network's certainty.
    Therefore, $\hat{y}$ indicates the likeliness of an input being a certain target class.
    In contrast to this, the \textit{identity activation function} is used to predict real numbers.
    By using the identity function, the \gls{nn} is equivalent to the \textit{least-squares regression} algorithm.
    An important property that all activation functions have in common is monotonicity and besides the identity function they also \textit{"saturate at large absolute values at which increasing further does not change the activation much."}\citep[\p{13}]{aggarwal_neural_2018}

    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{figures/various_activation_functions.png}
        \caption[Various activation functions and their relation between the domain and the codomain]{Various activation functions and their relation between the domain and the codomain\protect\footnotemark}
        \label{fig:activation_functions}
    \end{figure}

    \footnotetext{\cite[\p{13}]{aggarwal_neural_2018}}

    Activation functions are applied on the pre-activated value and therefore a formal notation of $\hat{y}$ is denoted by $\hat{y}=\phi(\bar{\mathbf{W}}\cdot\bar{\mathbf{X}})$.
    Over the course of the advancement of \gls{nn}, initial activation functions such as the Sigmoid and the \textit{Tangens-Hyperbolicus} activation function have been replace by piecewise linear functions such as the \textit{\gls{relu} activation function}.

    \begin{align*}
        &\text{Sigmoid: } &  \phi(v) & = \frac{1}{1+e^{-v}} \\
        &\text{Tangens-Hyperbolicus: } &  \phi(v) &= \frac{e^{2v}-1}{e^{2v}+1} \\
        &\text{\gls{relu}: } &  \phi(v)& = \vphantom{\frac11} \max\{v,0\}
    \end{align*}

    This is due to the fact that it facilitates the training process of \glspl{mlp} as they are less expensive to compute.

    \subsection{Loss Functions}
    In subsection~\ref{subsec:the-basic-architecture-of-neural-networks}, a generic loss function $E(\bar{\mathbf{X}})$ has been introduced.
    However, depending on the target and the prediction $\hat{y}$, another loss function may be better suited as certain loss functions are more sensitive to errors for classification than for predicting real numbers~\citep{aggarwal_neural_2018}.
    For instance, regression tasks modeled by \glspl{nn} with an identity activation function commonly use the \textit{squared loss} $(y - \hat{y})^2$ as an error measurement.
    In the case of classification tasks with multiple class labels, output values are normally activated using the \textit{softmax activation function}~\citep{goodfellow_deep_2016}, with $\mathbf{v}$ being the $n$ pre-activated output values.

    \begin{equation}
        \text{Softmax: } \sigma(\mathbf{v})_i = \frac{e^{v_i}}{\sum_{j=0}^{n}e^{v_j}}
    \end{equation}

    This function allows for a probabilistic output which means that it maps the pre-activated values to a probability distribution.
    Since the output of the softmax function is a vector of probabilities instead of a scalar value, a different loss function is needed as well.
    Additionally, classification tasks can have binary or categorical/multiple targets which further defines and differentiates which loss function will be used.

    For binary targets different combinations of activation and loss functions can be used to obtain the same result for prediction.
    For example, binary targets can be classified using \textit{logistic regression} which uses the identity activation function to predict $\hat{y}$.
    A loss function for this prediction is define by the following equation:

    \begin{equation}
        L=\text{log}(1+e^{-y\cdot\hat{y}})
    \end{equation}

    Another option is to simply use the sigmoid activation function which maps the output $\hat{y}$ between $0 \text{ and } 1$.
    Under the assumption that the observed value $y$ is normalized between $-1 \text{ and } 1$, the negative logarithm of $|\frac{y}{2}-\frac{1}{2}+\hat{y}|$ is used as a loss function.
    This loss function provides a measurement on how likely it is that the target has correctly been classified.

    Concerning categorical targets, post-activated values $\hat{y}_0,\ldots,\hat{y}_n$ are examined one by one.
    The loss for the $i$th prediction is given by the following equation:

    \begin{equation}
        L=-\text{log}(\hat{y}_i)
    \end{equation}

    This definition is known as the \textit{cross-entropy loss} and represents the extension of logistic regression~\citep{aggarwal_neural_2018}.
    %p. 15


    \section{Multi Layer Neural Networks}
    When comparing a simple perceptron to an \gls{mlp}, the major difference is the usage of multiple layers, so called \textit{hidden layers}.
    Instead of a perceptron where all computations are visible, hidden layers in an \gls{mlp} hide the computations~\citep{aggarwal_neural_2018}.
    However, the \gls{mlp} still is a feedforward network and the perceptron as well as the \gls{mlp} can both be described as a \textit{directed acyclic graph}.
    The additional layers in an \gls{mlp} can be described as a composition of multiple functions, in opposition to a perceptron where the input is directly transmitted to the output layer.
    By composing the additional layers via chain like structures, the input is feed through multiple layers before it allows a prediction of $\hat{y}$.
    For instance, an \gls{mlp} with three layers approximates the function $f(\bar{\mathbf{X}})$ by chaining three functions $f^{(3)}(f^{(2)}(f^{(1)}(\bar{\mathbf{X}})))$ together for a prediction~\citep{goodfellow_deep_2016}.
    %p. 163
    The amount of layers or rather the length of the chain is known as the depth of a \gls{nn}/an \gls{mlp} which also established the name \textit{deep learning} (\gls{dl}).
    As aforementioned in the introduction to the chapter~\nameref{ch:deep-reinforcement-learning}, only \glspl{mlp} with their ability to chain multiple functions allow for the approximation of a complex value function for a high dimensional state-space.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.75\textwidth]{figures/xor_problem.png}
        \caption[The XOR problem with the left plot being the original space using a perceptron and the right plot being the learned space using an \gls{mlp}]{The XOR problem with the left plot being the original space using a perceptron and the right plot being the learned space using an \gls{mlp}\protect\footnotemark}
        \label{fig:xor_problem}
    \end{figure}

    \footnotetext{\cite[\p{168}]{goodfellow_deep_2016}}
    The reason why \glspl{mlp} are the preferred choice instead of a perceptron is illustrated by an example of a \gls{nn} which should estimate a model for the logical operator XOR, the "exclusive or".
    In detail, the XOR functions takes two binary values $x_0,x_1$ and returns 1 if $x_1 \neq x_2$ and 0 otherwise.
    This described behaviour is also known as the target function $f^*(\bar{\mathbf{X}})$ of the XOR function, the \gls{nn} should approximate.
    Observing figure~\ref{fig:xor_problem}, the learned linear function of a perceptron would no be able correctly implement the XOR function.
    When applying the model learned by the perceptron to the left, original space, an approximated linear function cannot separate these values and thus would not be able to return the correct fot $y$.
    This is also known under the terme of \textit{linear separability}.
    However, after transforming the original space with the help of a hidden layer and a nonlinear activation function (e.g. \gls{relu}) to the space seen on the right side, the XOR function can be solved as the space is now linear separable~\citep{goodfellow_deep_2016,aggarwal_neural_2018}.
    This shows that an \gls{mlp} allows for much more complex function approximation as stated in the introduction to the chapter~\nameref{ch:deep-reinforcement-learning}.
    With the newly introduced capability, the learning process mentioned in equation \ref{eq:weight_adjusting} in section \ref{subsec:the-basic-architecture-of-neural-networks} is not applicable anymore.
    Instead, a new algorithm named \textit{backpropagation} is used which can be looked up either in \citet[\p{21}]{aggarwal_neural_2018} or in a more formal way in \citet[\p{197}]{goodfellow_deep_2016}.

    %TODO \subsection{Convolutional Neural Networks}


    %TODO \subsection{Variational Auto Encoders}

    \section{Deep Learning Approach in Reinforcement Learning}
    The advancements of \gls{dl} over the last couple of years, starting around 2010, allowed the domain of \gls{rl} to evolve as well. 
    Especially through the previously mentioned combination of these two fields and the newly creation of the \gls{drl} domain, many astonishing accomplishments were achived \citep{francois-lavet_introduction_2018}. 
    A summary of four different achivements which was selected by \citeauthor{aggarwal_neural_2018} is listed in the following.
    %p. 374

    \begin{itemize}
	    \item Video games are a classical example where \gls{drl} excelled over the course of the last few years.
		    Especially, games from the Atari 2600 gaming console are famous environments where \gls{drl} algorithm are applied, receiving only the raw pixels as an input.
		    Based on the pixels, a state representation is formed on which the agent takes actions, makes mistakes, gains experience and finally improves its decision making.
		    This procedure is very familiar to the traditional \gls{rl} approach known from chapter \ref{ch:reinforcement-learning-background}.
		    However, it is now applied to a much richer state-space and thanks to \gls{dl}, the agent is able to surpass humans in these games \citep{mnih_playing_2013,schulman_trust_2015}.
		    Furthermore, the reason why video games are even so popular in modern \gls{rl} is because they function as simulations of real life microcosms that are perfectly suitable for testing algorithms in advance before they are applied in the real world.

	    \item   Another very famous breakthrough happend in 2016, when an algorithm by the name \textit{AlphaGo} \citep{silver_mastering_2017} defeated the top-ranked players in the boardgame Go.
		    The reason why this event was so remarkable is because Go is a very complex game and being good at it takes a lot of human-like intuition.
		    Additionally, the state space is very large when compared to other board games such as chess.
		    However, it really was the unconventional learning process which made AlphaGo well known as it improved itself by obtaining experience by playing against itself.

	    \item   \gls{drl} is also viewd as a viable option for self-driving cars.
		    Although a more common approach to this problem is the utilization of supervised learning, the usage of various carsensors for decision making allows these automated cars to have a lower error rate when compared to humans.
	    \item   Another important domain where \gls{drl} plays a key factor is the creation of self-learning robots.
		    Here, the difficulty relies in teaching a robot basic human-like motions such as walking.
		    Fortunately, the task of teaching a robot to walk can be described in the \gls{rl} framework as the robots goal is to reach a given position as fast as possible.
		    The robot's state is a description of its available limbs and motors in a continous space.
		    Therefore, \gls{dl} techniqes must be applied as well to reach good performance.
		    This does not only allow the robot to learn how to walk but also how to roll and crawl.
    \end{itemize}

    All of these acomplishments are based on different \gls{drl} approaches. 
    However, an exaustive explanation of each of these algorithms would go beyond the scope of this thesis. Therefore, solely an overview is provided by figure \ref{fig:drl_taxonomy}.
    For further infromations the paper by \citeauthor{francois-lavet_introduction_2018} is recommended.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{figures/drl_taxonomy.png}
	    \caption[A non-exaustive taxonomy of \gls{drl} algorithms]{A non-exaustive taxonomy of \gls{drl} algorithms\protect\footnotemark}
        \label{fig:drl_taxonomy}
    \end{figure}

    \footnotetext{\url{https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html}}

%    \section{Trust Region Policy Optimization}\label{sec:trust-region-policy-optimization}


    \section{Proximal Policy Optimization}\label{sec:proximal-policy-optimization}
    \cite{schulman_proximal_2017}


    \glsresetall
    \chapter{Intrinsic Motivation in Reinforcement Learning}


    \section{Knowledge Acquisition}


    \glsresetall
    \chapter{Evalutation}

    %TODO chapter of an introduction to drl for evalutation


    \glsresetall
    \chapter{Implementation}


    \glsresetall
    \chapter{Results}
    
    \glsresetall
    \chapter{Discussion}

    \backmatter
% Use an optional list of figures.
    \listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
    \cleardoublepage % Start list of tables on the next empty right hand page.
    \listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of alogrithms.
    \listofalgorithms
    \addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
    \printindex

% Add a glossary.
    \printglossaries

% Add a bibliography.
    \bibliographystyle{apalike}
    \bibliography{core}

\end{document}
