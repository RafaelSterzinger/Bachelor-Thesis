% Copyright (C) 2014-2020 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesettings of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{hyperref}  % Enables cross linking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists fo acronyms. This package has to be included last.
\usepackage{natbib}

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Rafael Sterzinger} % The author name without titles.
\newcommand{\thesistitle}{Balancing Extrinsic and Intrinsic Rewards in Reinforcement Learning}
%Prediction Error based Curiosity and How to Combine Extrinsic and Intrinsic Rewards
\newcommand{\thesissubtitle}{}% The title of the thesis. The English version should be used, if it exists.
\newcommand{\p}[1]{see p. #1}

% Set PDF document properties
\hypersetup{
pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
linkbordercolor = {Melon},                % The color of the borders of boxes around crosslinks (optional).
pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
pdfsubject      = {Subject},              % The document's subject in the document properties (optional).
pdfkeywords     = {a, list, of, keywords} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph identation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{}{male}
\setauthorextra
\setadvisor{Ao.Univ.Prof. Dipl.-Ing. Dr.techn.}{Thomas Grechenig}{}{male}

% For bachelor and master theses:
\setfirstassistant{Dipl.-Ing.}{Michael Ressmann}{Posttitle}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
%\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{11778282}
\setdate{10}{08}{2020} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{\thesistitle} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
\setsubtitle{\thesissubtitle}{\thesissubtitle} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor / phd-school.
% Bachelor:
\setthesis{bachelor}
%
% Master:
%\setthesis{master}
%\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.
%
% Doctor at the PhD School
%\setthesis{phd-school} % Deactivate non-English title pages (see below)

% For bachelor and master:
\setcurriculum{Business Informatics }{Wirtschaftsinformatik} % Sets the English and German name of the curriculum.

% For dissertations at the PhD School:
%\setfirstreviewerdata{Affiliation, Country}
%\setsecondreviewerdata{Affiliation, Country}


\newacronym{im}{IM}{Intrinsic Motivation}
\newacronym{rl}{RL}{Reinforcement Learning}
\newacronym{mdp}{MDP}{Markov Decision Process}
\newacronym{ml}{ML}{Machine Learning}
\newacronym{drl}{DRL}{Deep Reinforcement Learning}



\begin{document}

    \frontmatter % Switches to roman numbering.
%  The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

    \addtitlepage{naustrian} % German title page (not for dissertations at the PhD School).
    \addtitlepage{english} % English title page.
    \addinsotitlepage{naustrian}
    \addstatementpage

    \begin{danksagung*}
    \end{danksagung*}

    \begin{acknowledgements*}
    \end{acknowledgements*}

    \begin{kurzfassung}
    \end{kurzfassung}

    \begin{abstract}
    \end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
    \selectlanguage{english}

% Add a table of contents (toc).
    \tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
    \mainmatter


    \chapter{Introduction}


    \section{Problem Description}\label{sec:problem-description}
    In \gls{rl}, algorithms mainly depend on carefully designed extrinsic reward functions which are comparable with feedback to an agent's behaviour.
    In order to teach an agent a certain desired behaviour, it has to optimize this reward function by trial-and-error.
    This traditional approach yielded multiple astonishing results over the last decade.
    Task which award rewards densely, i.e.\ almost after every performed action, are common for these results.
    In opposite to dense rewards, sparse rewards are scattered in the environment which poses the problem that they occur too rarely in order for an agent to pick up the needed skills.
    An example for environments with sparse rewards are tasks which depend heavily on exploration, e.g. \textit{Montezumas's revenge}.
    \\\\
    An approach to tackle these problems is curiosity, an organism's \gls{im} to spontaneously explore its environment.
    IM allows an agent to incrementally learn valuable skills independently of its given task by an intrinsic and therefore, more general reward function.
    In general, the concept of intrinsic and extrinsic evolves around the question on why an agent performed a certain action in a given state.
    Since IM became an important part of RL, the following challenges were crystallized out by~\cite[\p{6}]{aubret_survey_2019}:

    \begin{itemize}
        \item \textbf{Sparse rewards:} The agent never reaches a reward signal in case of sparse rewards.
        \item \textbf{State representation:} The agent does not manage to learn a representation of its observations with independent features or meaningful distance metrics.
        \item \textbf{Building option:} The agent is unable to learn abstract high-level decisions independently from the task.
        \item \textbf{Learning a curriculum:} The agent hardly defines a curriculum among its available goals without expert knowledge.
    \end{itemize}

    Besides a general introduction to IM and its challenges in RL, this Bachelor's thesis has its focus on knowledge acquisition through exploration.
    Knowledge acquisition is described as the agents motivation to find new knowledge about its environment, meaning that it is interested in things it can or cannot control, the function of the world, discovering new areas, or understanding the proximity.
    Concerning exploration, one approach is error prediction which is the agents difficulty to predict the state following a state-action-tuple.
    This idea is heavily explored by~\cite{burda_large-scale_2018} and its prior work of \cite{pathak_curiosity-driven_2017}, both of which are core references of this thesis.
    \\\\
    Building upon the findings of~\citeauthor{burda_large-scale_2018}, this Bachelor's thesis eyes on answering the question on how to optimally combine intrinsic and extrinsic rewards in order to maximize an agents score, primarily in sparse but also in dense environments.
    Furthermore, it will evaluate if a balanced reward combination tackles the noisy-TV problem, proposed by~\cite{schmidhuber_formal_2010}.
    The noisy-TV problem, also known as the white-noise problem, is an algorithms inability to handle the local stochasticity of the environment: "\ldots random noise in a 3D environment attracts the agent; it will passively watch the noise since it will not be able to predict the next state."\cite[\p{10}]{aubret_survey_2019}
    \\\\
    The domain of the given problem description is IM in RL with a focus on the model of knowledge acquisition via exploration.


    \section{Expected Results}\label{sec:expected-results}
    As aforementioned, the goal pursued by this Bachelor's thesis is to build upon the results of~\citeauthor{burda_large-scale_2018} and to pursue their posed question on how to optimally combine intrinsic ($r_{int}$) and extrinsic ($r_{ext}$) rewards.
    In order to do so, coefficients $\alpha$ and $\beta$ have to be selected accordingly.
    \[r=\alpha r_{int} + \beta r_{ext}\]
    From that, an optimal solution to this equation maximizes the agent's overall reward when testing.
    After a re-implementation of the proposed algorithms, the benchmarks are expected to at least match the mean rewards mentioned in the \textit{Additional Results} section from the underlying paper.
    In this section, the authors explored the performance of combined rewards on five different Atari games and already showed that the combination of rewards yield a higher mean reward.
    Therefore, an exhaustive hyper-parameter tuning should certainly improve these results.
    \\\\
    However, this raises the question on how the algorithm performs in comparison to comparable approaches such as the paper authored by~\cite{kim_emi_2019}.
    In some of the previously mentioned hard games, e.g. \textit{Frostbite}, the authors achieved only a slightly higher mean score which might be beatable by combining rewards.
    Optionally, it would be interesting to observe the performance in environments with dense rewards too since there might be an improvement to see as well.
    \\\\
    In the \textit{Discussion} section,~\citeauthor{burda_large-scale_2018} mention the limitations of prediction error based curiosity which was earlier introduced with the so called noisy-TV problem.
    Given this problem, the question is raised on whether or not it is possible to tackle the agent's distraction through curiosity with extrinsic motivation.
    This question will be pursued during the evaluation process with the posed hypothesized that it is possible to overcome an agent's distraction and the assumption that extrinsic motivation has a higher impact on the distraction in a densely rewarding environment than in a sparse one.
    \\\\
    With the given prospect to beat state of the art approaches to environments with sparse rewards and evaluate the amount of extrinsic reward needed to put a distracted agent back on track, it is the opportunity to pursue important constructs, which reflect the natural human propensity to learn and assimilate, in the domain of computer science that motivated this Bachelor's thesis.


    \section{Methodological Approach}\label{sec:methodological-approach}
    In order to answer the question on how to optimally combine intrinsic and extrinsic rewards, the thesis builds upon the released source code and environments from~\cite{burda_large-scale_2018} and~\cite{pathak_curiosity-driven_2017} which can be observed on these websites. \footnote{\url{https://pathak22.github.io/large-scale-curiosity}}\footnote{\url{https://pathak22.github.io/noreward-rl/}}
    Therefore, this Bachelor's thesis follows a programming approach which relies on the same tools used by the preceding authors, since it guarantees optimal reproducibility of their results.
    In an overview, these tools are the \textit{Python}\footnote{\url{https://www.python.org/}} programming language, the machine learning platform \textit{TensorFlow}\footnote{\url{https://www.tensorflow.org/}}, and lastly the \textit{OpenAI Gym}\footnote{\url{https://gym.openai.com/}} which is a toolkit for developing and comparing RL algorithms.
    \\\\
    The development progress is structured into the following five mile stones:

    \begin{enumerate}
        \item Setting-up of the environment, including the installation of the \textit{CUDA}\footnote{\url{https://developer.nvidia.com/cuda-toolkit}} toolkit and the \textit{cuDNN}\footnote{\url{https://developer.nvidia.com/cudnn}]} library
        \item Implementing a Proximal Policy Optimization (PPO) algorithm according to~\cite{schulman_proximal_2017}, using extrinsic rewards
        \item Adding an Intrinsic Curiosity Module, published by~\cite{pathak_curiosity-driven_2017}, in order to allow for intrinsic rewards
        \item Tuning of the coefficients $\alpha,\beta$ and comparison to state of the art approaches
        \item Recreation of the synthetic generated noisy-TV problem, proposed by~\cite{burda_large-scale_2018}
    \end{enumerate}

    During this incremental process, an emphasise is laid on two out of five sparse Atari games, categorized by~\cite{bellemare_unifying_2016} and picked by~\citeauthor{burda_large-scale_2018}.
    The initial two games are \textit{Montezumas's revenge} and \textit{Freeway}, but with the option to add the remaining ones or games with dense rewards in the future.
    \\\\
    % Eventually add VAE
    Regarding mile stone three, \textit{Random Features} and \textit{Inverse Dynamics Features} as introduced by~\citeauthor{burda_large-scale_2018} are primarily the focus.
    Concerning mile stone four and the comparison to state of the art approaches, the~\nameref{sec:state-of-the-art} section of this thesis allows for a current in-depth overview.
    Additionally to the aforementioned practical part, the theoretical aspect of this Bachelor's thesis will be covered by an exhaustive literature research with an emphasize on directly comparable approaches, i.e. prediction error based curiosity~\citep{burda_large-scale_2018}.


    \section{State of the Art}\label{sec:state-of-the-art}


    \section{Thesis Outline}\label{sec:thesis-outline}


    \chapter{Reinforcement Learning Background}\label{ch:reinforcement-learning-background}


    \section{Reinforcement Learning Problem}\label{sec:reinforcement-learning-problem}
    Reinforcement Learning as denoted by~\cite{sutton_reinforcement_2018} and sometimes also referred to as the science of decision making, is about learning the consequences of performed actions and using this knowledge to maximize a numeric reward signal.
    RL problems are tackled through a learner or rather an agent which performs an actions in an environment in a trial and error paradigm.
    The agent has no prior knowledge of the consequences of the possible actions but has to explore which action to take in a certain situation to obtain the highest reward and creates the need for a proper weighting between exploration and exploitation.
    Additionally, the consequences of an action can be delayed over time and might not only affect the immediate reward, but affect all future rewards as well.

    In opposite to other known machine learning disciplines such as \textit{supervised learning}, there is no supervisor in RL that explicitly tells the agent which action would be correct in a given state and because of that, the agent has to come up with its own policy based on the repeated feedback it receives from the immediate reward and the following state~\citep{kaelbling_reinforcement_1996}.
    This spares the necessity of creating a trainings dataset with corresponding state-action pairs beforehand and thus, RL is the preferred choice in cases where an agent interacts with its environment.
    Furthermore, teaching an agent based on supervision would mean that it would be limited to the labelling ability of the supervisor which is another reason why it must be able to learn from its own experiences~\citep{silver_rl_nodate}.
    Since RL is quite distinct from supervised learning and does not rely on labelled examples, it is sometimes misclassified as \textit{unsupervised learning} which has the purpose to find structure in unlabelled data.
    However, this is not applicable for RL because the agent has to maximize a reward function and not to find structure in data and therefore, RL is rightfully classified as a third subfield of \gls{ml}~\citep{sutton_reinforcement_2018}.


    \section{Markov Chains}
    In order to formalize the RL problem, the idea and notation of a \gls{mdp} is used to notate the interactions between an agent and its environment.

    In order to know which actions can be performed at a certain time, the learner must have the ability to observe the environment's current state and to change it by interacting with it.


    \section{Markov Decision Processes}


    \section{Dynamic Programming}


    \section{Functions to Improve Behaviour}


    \section{Temporal Difference Leraning}

    \subsection{Q-Learning (Off-Policy)}

    \subsection{SARSA (On-Policy)}

    \subsection{Temporal Difference}


    \chapter{Deep Reinforcement Learning}


    \chapter{Intrinsic Motivation in Reinforcement Learning}


    \chapter{Knowledge Acquisition}


    \chapter{Evalutation}


    \chapter{Implementation}


    \chapter{Results and Discussion}


% Remove following line for the final thesis.
%\input{intro.tex} % A short introduction to LaTeX.

    \backmatter
% Use an optional list of figures.
    \listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
    \cleardoublepage % Start list of tables on the next empty right hand page.
    \listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of alogrithms.
    \listofalgorithms
    \addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
    \printindex

% Add a glossary.
    \printglossaries

% Add a bibliography.
    \bibliographystyle{apalike}
    \bibliography{core}

\end{document}
